{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9n0lA-t61oH",
        "outputId": "7bc3ca01-3aeb-4ef4-8e6e-041eccae8ae0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AAqdjC5TH6-"
      },
      "source": [
        "import os, os.path, shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import pathlib\n",
        "import os\n",
        "import datetime\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.image import *\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji8yZVOYuS_I"
      },
      "source": [
        "## Data acquisition and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuuytq0drD52"
      },
      "source": [
        "#### Download and load food-101 dataset, excluding worst food classes which were previously determined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmDhaXCi4HS7"
      },
      "source": [
        "# Splits dataset into training folder and test folder\n",
        "\n",
        "!wget https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
        "!tar -xzf food-101.tar.gz\n",
        "data_dir = pathlib.Path(\"/content/food-101/images\")\n",
        "val_dir = pathlib.Path(\"/content/validation\")\n",
        "test_dir = pathlib.Path(\"/content/test\")\n",
        "\n",
        "exception = [42, 18, 84, 3, 17, 47, 80, 36, 26, 89, 96, 39, 5, 49, 57, 59, 99, 8, 10, 0, 15, 82, 56, 67, 37, 93, 22, 50, 4, 87, 77]\n",
        "\n",
        "folder_path = data_dir\n",
        "train_path = test_dir\n",
        "\n",
        "folders = sorted([f for f in os.listdir(folder_path)])\n",
        "\n",
        "for index, folder in enumerate(folders):\n",
        "  path = os.path.join(folder_path, folder)\n",
        "  tpath = os.path.join(train_path,folder)\n",
        "\n",
        "  if index in exception:\n",
        "    shutil.rmtree(path)\n",
        "    continue\n",
        "\n",
        "  if not os.path.exists(tpath):\n",
        "      os.makedirs(tpath)\n",
        "\n",
        "  images = os.listdir(path)\n",
        "  count = 0\n",
        "  for image in images:\n",
        "    if count <= 100:\n",
        "      old_image_path = os.path.join(path,image)\n",
        "      new_image_path = os.path.join(tpath,image)\n",
        "      shutil.move(old_image_path, new_image_path)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZhAb4gRb37A"
      },
      "source": [
        "# Split again to create validation set\n",
        "\n",
        "folder_path = data_dir\n",
        "val_path = val_dir\n",
        "\n",
        "folders = [f for f in os.listdir(folder_path)]\n",
        "\n",
        "for folder in folders:\n",
        "    path = os.path.join(folder_path, folder)\n",
        "    tpath = os.path.join(val_path,folder)\n",
        "\n",
        "    if not os.path.exists(tpath):\n",
        "        os.makedirs(tpath)\n",
        "\n",
        "    images = os.listdir(path)\n",
        "    count = 0\n",
        "    for image in images:\n",
        "      if count <= 100:\n",
        "        old_image_path = os.path.join(path,image)\n",
        "        new_image_path = os.path.join(tpath,image)\n",
        "        shutil.move(old_image_path, new_image_path)\n",
        "      count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xshWyZZseDF"
      },
      "source": [
        "#### Define data generators to apply transformations to data, and create train and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO0QwRpeKVuW",
        "outputId": "200f4846-e6d3-4bc3-b107-f8f0a68aeeeb"
      },
      "source": [
        "batch_size = 64\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "\n",
        "train_img_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=5,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=False,\n",
        "                                   fill_mode='nearest',\n",
        "                                   zoom_range=0.2\n",
        "                                  )\n",
        "\n",
        "val_img_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_ds = train_img_data_gen.flow_from_directory(directory=data_dir,\n",
        "                                                    class_mode='sparse',\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True\n",
        "                                                   )\n",
        "\n",
        "val_ds = val_img_data_gen.flow_from_directory(directory=val_dir,\n",
        "                                                    class_mode='sparse',\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True\n",
        "                                                   )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 62930 images belonging to 70 classes.\n",
            "Found 7070 images belonging to 70 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAZgZ5UAsR9J"
      },
      "source": [
        "#### Visualize first 5 preprocessed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRtXejQ6kziB"
      },
      "source": [
        "x_batch, y_batch = next(train_ds)\n",
        "\n",
        "for i in range(5):\n",
        "    image = x_batch[i]\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAcn44bIuYao"
      },
      "source": [
        "## Model creation and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs791k2-sqPF"
      },
      "source": [
        "#### Create model using pretrained portion of InceptionResNetV2, and freeze pretrained layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6AO7K4YOaAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f1bd48-02af-4966-edb7-63ebe9be9413"
      },
      "source": [
        "num_classes = 70\n",
        "\n",
        "modelBase = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(256, 256, 3), pooling='avg')\n",
        "\n",
        "modelp = modelBase.output\n",
        "modelp = Dense(2048, kernel_initializer='he_uniform', activation='relu')(modelp)\n",
        "output = Dense(num_classes, activation='softmax')(modelp)\n",
        "\n",
        "model = Model(inputs=modelBase.input, outputs=output)\n",
        "\n",
        "for layer in modelBase.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbrwQ0GBv6ai"
      },
      "source": [
        "# Unfreeze top few layers\n",
        "\n",
        "for layer in model.layers[:777]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[777:]:\n",
        "   layer.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyzi_8P-tSba"
      },
      "source": [
        "#### Define learning rate and begin training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcxhuFJLOm0V"
      },
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(\n",
        "  optimizer=opt,\n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(),\n",
        "  metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCkYdnvtOoqk"
      },
      "source": [
        "# Saves model weights when new validation low observed.\n",
        "# Learning rate schedule will reduce learnign rate by 0.1\n",
        "# after 5 epochs of no val_loss improvement.\n",
        "\n",
        "print(\"Start training:\",datetime.datetime.now())\n",
        "\n",
        "modelPath = \"/content/drive/MyDrive/MAIS/MAIS202Data/inception-new3\"\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=30, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint(modelPath, save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='min')\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    initial_epoch=0,\n",
        "    max_queue_size=100,\n",
        "    workers=300,\n",
        "    use_multiprocessing=False,\n",
        "    callbacks=[earlyStopping, mcp_save, reduce_lr_loss]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSW7UWJeuAGf"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytUBo9T8BebE"
      },
      "source": [
        "# Preprocess test dataset\n",
        "\n",
        "test_ds = val_img_data_gen.flow_from_directory(directory=test_dir,\n",
        "                                                    class_mode='sparse',\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True\n",
        "                                                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNNwF-kRAgQV",
        "outputId": "7488a52a-2bd2-462c-ee79-7fd8b88a9b3b"
      },
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(test_ds, verbose=1)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "111/111 [==============================] - 33s 297ms/step - loss: 0.2155 - sparse_categorical_accuracy: 0.9375\n",
            "test loss, test acc: [0.21546627581119537, 0.9374822974205017]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9s4uDc0urw1"
      },
      "source": [
        "## Convert model to .mlmodel and download to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi4Z6gFaM3zM"
      },
      "source": [
        "!pip install coremltools==4.0\n",
        "import coremltools as ct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPuWAEQ9M9UH"
      },
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "model.save('ImageClassifier.h5')\n",
        "\n",
        "output_labels = list(test_ds.class_indices.keys())\n",
        "\n",
        "classifier_config = ct.ClassifierConfig(output_labels)\n",
        "\n",
        "image_input = ct.ImageType(shape=(1, 256, 256, 3,),\n",
        "                           bias=[-1,-1,-1], scale=2.0/225.0)\n",
        "\n",
        "image_classifier = ct.convert('ImageClassifier.h5', inputs=[image_input], classifier_config=classifier_config)\n",
        "\n",
        "image_classifier.short_description = 'Classification of 70 different foods'\n",
        "image_classifier.input_description[\"input_1\"] = \"Input image to be classified\"\n",
        "image_classifier.output_description[\"classLabel\"] = \"Most likely image category\"\n",
        "\n",
        "image_classifier.save('ImageClassifier.mlmodel')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}